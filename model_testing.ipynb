{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interactive Worlds - Model & Prompt Testing\n",
        "\n",
        "This notebook allows you to test different models and prompts for the Interactive Worlds game.\n",
        "\n",
        "**Features:**\n",
        "- OpenRouter Responses API with reasoning (effort: high)\n",
        "- Thinking output exposed for debugging\n",
        "- Streaming support with real-time reasoning display\n",
        "- Compare different models and prompts\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, make sure you have your OpenRouter API key in the `.env` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if needed\n",
        "!pip install openai python-dotenv requests -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Get API key\n",
        "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "if not OPENROUTER_API_KEY:\n",
        "    raise ValueError(\"OPENROUTER_API_KEY not found in .env file\")\n",
        "\n",
        "# Initialize OpenRouter client for Chat Completions API (legacy)\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        ")\n",
        "\n",
        "print(\"‚úì Setup complete!\")\n",
        "print(f\"‚úì API Key: {OPENROUTER_API_KEY[:10]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Available Models\n",
        "\n",
        "Configure which models you want to test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configurations\n",
        "MODELS = {\n",
        "    # Free tier\n",
        "    \"gemini-flash-free\": \"google/gemini-2.0-flash-exp:free\",\n",
        "    \"claude-haiku\": \"anthropic/claude-3.5-haiku\",\n",
        "    \n",
        "    # Pro tier (with extended thinking support)\n",
        "    \"gemini-pro\": \"google/gemini-exp-1206\",\n",
        "    \"claude-sonnet\": \"anthropic/claude-sonnet-4\",\n",
        "    \n",
        "    # Alternative models for testing\n",
        "    \"gpt-4o\": \"openai/gpt-4o\",\n",
        "    \"gpt-4o-mini\": \"openai/gpt-4o-mini\",\n",
        "    \"o1-mini\": \"openai/o1-mini\",\n",
        "    \"llama-70b\": \"meta-llama/llama-3.1-70b-instruct\",\n",
        "}\n",
        "\n",
        "# Models that support extended thinking/reasoning\n",
        "REASONING_MODELS = [\n",
        "    \"claude-haiku\",\n",
        "    \"claude-sonnet\",\n",
        "    \"o1-mini\",\n",
        "    \"gemini-pro\",\n",
        "]\n",
        "\n",
        "# Print available models\n",
        "print(\"Available models:\")\n",
        "for name, model_id in MODELS.items():\n",
        "    reasoning_support = \"‚úì Reasoning\" if name in REASONING_MODELS else \"\"\n",
        "    print(f\"  - {name}: {model_id} {reasoning_support}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## System Prompt\n",
        "\n",
        "Load the system prompt from the project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read system prompt from file\n",
        "with open('lib/prompts/system-prompt.ts', 'r') as f:\n",
        "    content = f.read()\n",
        "    # Extract the prompt from the TypeScript export\n",
        "    start = content.find('`') + 1\n",
        "    end = content.rfind('`')\n",
        "    SYSTEM_PROMPT = content[start:end]\n",
        "\n",
        "print(f\"System prompt loaded: {len(SYSTEM_PROMPT)} characters\")\n",
        "print(\"\\nFirst 500 characters:\")\n",
        "print(SYSTEM_PROMPT[:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenRouter Responses API Functions\n",
        "\n",
        "Functions for using the Responses API with reasoning support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_with_reasoning(\n",
        "    model_name: str,\n",
        "    user_prompt: str,\n",
        "    system_prompt: str = SYSTEM_PROMPT,\n",
        "    effort: str = \"high\",\n",
        "    max_tokens: int = 9000,\n",
        "    stream: bool = True,\n",
        "    show_reasoning: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Test a model using OpenRouter's Responses API with reasoning.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name from MODELS dict or full model ID\n",
        "        user_prompt: The user message\n",
        "        system_prompt: The system prompt\n",
        "        effort: Reasoning effort level (minimal, low, medium, high)\n",
        "        max_tokens: Maximum tokens to generate\n",
        "        stream: Whether to stream the response\n",
        "        show_reasoning: Whether to display thinking/reasoning output\n",
        "    \n",
        "    Returns:\n",
        "        Dict with response, reasoning, and metadata\n",
        "    \"\"\"\n",
        "    # Get model ID\n",
        "    model_id = MODELS.get(model_name, model_name)\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing: {model_name} ({model_id})\")\n",
        "    print(f\"Reasoning Effort: {effort.upper()}, Max Tokens: {max_tokens}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Prepare request payload\n",
        "    payload = {\n",
        "        \"model\": model_id,\n",
        "        \"input\": [\n",
        "            {\n",
        "                \"type\": \"message\",\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [{\"type\": \"input_text\", \"text\": system_prompt}]\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"message\",\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [{\"type\": \"input_text\", \"text\": user_prompt}]\n",
        "            }\n",
        "        ],\n",
        "        \"reasoning\": {\"effort\": effort},\n",
        "        \"max_output_tokens\": max_tokens,\n",
        "        \"stream\": stream,\n",
        "    }\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        if stream:\n",
        "            # Streaming response\n",
        "            response = requests.post(\n",
        "                \"https://openrouter.ai/api/v1/responses\",\n",
        "                headers=headers,\n",
        "                json=payload,\n",
        "                stream=True,\n",
        "            )\n",
        "            \n",
        "            full_response = \"\"\n",
        "            reasoning_steps = []\n",
        "            \n",
        "            print(\"üìù Response:\\n\")\n",
        "            \n",
        "            for line in response.iter_lines():\n",
        "                if line:\n",
        "                    line_text = line.decode('utf-8')\n",
        "                    if line_text.startswith('data: '):\n",
        "                        data = line_text[6:]\n",
        "                        if data == '[DONE]':\n",
        "                            break\n",
        "                        \n",
        "                        try:\n",
        "                            parsed = json.loads(data)\n",
        "                            \n",
        "                            # Handle reasoning deltas\n",
        "                            if parsed.get('type') == 'response.reasoning.delta':\n",
        "                                if show_reasoning:\n",
        "                                    delta_text = parsed.get('delta', '')\n",
        "                                    reasoning_steps.append(delta_text)\n",
        "                                    print(f\"\\nüß† Thinking: {delta_text}\", flush=True)\n",
        "                            \n",
        "                            # Handle content deltas\n",
        "                            elif parsed.get('type') == 'response.output_item.delta':\n",
        "                                for item in parsed.get('delta', {}).get('content', []):\n",
        "                                    if item.get('type') == 'output_text':\n",
        "                                        text = item.get('text', '')\n",
        "                                        print(text, end='', flush=True)\n",
        "                                        full_response += text\n",
        "                        \n",
        "                        except json.JSONDecodeError:\n",
        "                            continue\n",
        "            \n",
        "            print(\"\\n\")  # New line after streaming\n",
        "            \n",
        "        else:\n",
        "            # Non-streaming response\n",
        "            response = requests.post(\n",
        "                \"https://openrouter.ai/api/v1/responses\",\n",
        "                headers=headers,\n",
        "                json=payload,\n",
        "            )\n",
        "            \n",
        "            result = response.json()\n",
        "            \n",
        "            # Extract reasoning\n",
        "            reasoning_steps = []\n",
        "            full_response = \"\"\n",
        "            \n",
        "            for output_item in result.get('output', []):\n",
        "                if output_item.get('type') == 'reasoning':\n",
        "                    reasoning_steps = output_item.get('summary', [])\n",
        "                    if show_reasoning:\n",
        "                        print(\"üß† Reasoning Steps:\")\n",
        "                        for i, step in enumerate(reasoning_steps, 1):\n",
        "                            print(f\"  {i}. {step}\")\n",
        "                        print()\n",
        "                \n",
        "                elif output_item.get('type') == 'message':\n",
        "                    for content_item in output_item.get('content', []):\n",
        "                        if content_item.get('type') == 'output_text':\n",
        "                            full_response = content_item.get('text', '')\n",
        "            \n",
        "            print(\"üìù Response:\\n\")\n",
        "            print(full_response)\n",
        "        \n",
        "        end_time = datetime.now()\n",
        "        duration = (end_time - start_time).total_seconds()\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Response length: {len(full_response)} characters\")\n",
        "        print(f\"Reasoning steps: {len(reasoning_steps)}\")\n",
        "        print(f\"Time taken: {duration:.2f} seconds\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        return {\n",
        "            \"response\": full_response,\n",
        "            \"reasoning\": reasoning_steps,\n",
        "            \"duration\": duration,\n",
        "            \"model\": model_id,\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "print(\"‚úì Reasoning API functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Legacy Chat Completions API Function\n",
        "\n",
        "For models that don't support reasoning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model_legacy(\n",
        "    model_name: str,\n",
        "    user_prompt: str,\n",
        "    system_prompt: str = SYSTEM_PROMPT,\n",
        "    temperature: float = 0.9,\n",
        "    max_tokens: int = 8000,\n",
        "    stream: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Test a model using legacy Chat Completions API (no reasoning).\n",
        "    \"\"\"\n",
        "    model_id = MODELS.get(model_name, model_name)\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing: {model_name} ({model_id})\")\n",
        "    print(f\"Temperature: {temperature}, Max Tokens: {max_tokens}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    try:\n",
        "        if stream:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model_id,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                ],\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "                stream=True,\n",
        "            )\n",
        "            \n",
        "            full_response = \"\"\n",
        "            for chunk in response:\n",
        "                if chunk.choices[0].delta.content:\n",
        "                    content = chunk.choices[0].delta.content\n",
        "                    print(content, end='', flush=True)\n",
        "                    full_response += content\n",
        "            \n",
        "            print()\n",
        "            \n",
        "        else:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model_id,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                ],\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "            )\n",
        "            \n",
        "            full_response = response.choices[0].message.content\n",
        "            print(full_response)\n",
        "        \n",
        "        end_time = datetime.now()\n",
        "        duration = (end_time - start_time).total_seconds()\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Response length: {len(full_response)} characters\")\n",
        "        print(f\"Time taken: {duration:.2f} seconds\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        return full_response\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úì Legacy API functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## World Generation Prompts\n",
        "\n",
        "Test different world generation scenarios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example world generation prompts\n",
        "WORLD_PROMPTS = {\n",
        "    \"medieval_fantasy\": \"\"\"\n",
        "Please generate a complete world with the following parameters:\n",
        "- World Type: Medieval Fantasy\n",
        "- Power System: Hard Magic with strict scientific-like rules\n",
        "- Starting Class: Peasant\n",
        "- Additional Instructions: Create a rich, detailed world with complex political intrigue\n",
        "\n",
        "Generate the complete story bible as specified in the system prompt, then begin my adventure.\n",
        "\"\"\",\n",
        "    \n",
        "    \"cultivation\": \"\"\"\n",
        "Please generate a complete world with the following parameters:\n",
        "- World Type: Cultivation/Progression Fantasy (Xianxia style)\n",
        "- Power System: Cultivation with multiple realms and bottlenecks\n",
        "- Starting Class: Mortal with no cultivation base\n",
        "- Additional Instructions: Include hidden sects, secret techniques, and deadly competition for resources\n",
        "\n",
        "Generate the complete story bible as specified in the system prompt, then begin my adventure.\n",
        "\"\"\",\n",
        "    \n",
        "    \"scifi\": \"\"\"\n",
        "Please generate a complete world with the following parameters:\n",
        "- World Type: Science Fiction with multiple planet civilizations\n",
        "- Power System: Technology-based advancement and augmentations\n",
        "- Starting Class: Low-level station worker\n",
        "- Additional Instructions: Include galactic politics, corporate conspiracies, and AI threats\n",
        "\n",
        "Generate the complete story bible as specified in the system prompt, then begin my adventure.\n",
        "\"\"\",\n",
        "    \n",
        "    \"alternate_history\": \"\"\"\n",
        "Please generate a complete world with the following parameters:\n",
        "- World Type: Alternate History Earth (1800s with magic)\n",
        "- Power System: Soft magic with narrative flexibility\n",
        "- Starting Class: Apprentice in a trading company\n",
        "- Additional Instructions: Blend historical accuracy with magical elements, include colonial tensions\n",
        "\n",
        "Generate the complete story bible as specified in the system prompt, then begin my adventure.\n",
        "\"\"\",\n",
        "}\n",
        "\n",
        "print(\"Available world prompts:\")\n",
        "for name in WORLD_PROMPTS.keys():\n",
        "    print(f\"  - {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Claude with High Reasoning Effort\n",
        "\n",
        "Test Claude models with extended thinking visible:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure test\n",
        "MODEL_TO_TEST = \"claude-sonnet\"  # or \"claude-haiku\"\n",
        "PROMPT_TO_TEST = \"medieval_fantasy\"\n",
        "\n",
        "# Run test with reasoning\n",
        "result = test_with_reasoning(\n",
        "    model_name=MODEL_TO_TEST,\n",
        "    user_prompt=WORLD_PROMPTS[PROMPT_TO_TEST],\n",
        "    effort=\"high\",  # Use high reasoning effort\n",
        "    max_tokens=9000,\n",
        "    stream=True,\n",
        "    show_reasoning=True,  # Show thinking output\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Compare Reasoning Effort Levels\n",
        "\n",
        "Test how different reasoning effort levels affect the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "effort_levels = [\"minimal\", \"low\", \"medium\", \"high\"]\n",
        "model = \"claude-haiku\"\n",
        "prompt = WORLD_PROMPTS[\"medieval_fantasy\"]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for effort in effort_levels:\n",
        "    print(f\"\\n\\n‚ö° Testing effort level: {effort.upper()}...\\n\")\n",
        "    result = test_with_reasoning(\n",
        "        model_name=model,\n",
        "        user_prompt=prompt,\n",
        "        effort=effort,\n",
        "        max_tokens=4000,\n",
        "        stream=False,\n",
        "        show_reasoning=True,\n",
        "    )\n",
        "    results[effort] = result\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EFFORT LEVEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "for effort, result in results.items():\n",
        "    if result:\n",
        "        print(f\"\\n{effort.upper()}:\")\n",
        "        print(f\"  - Response length: {len(result['response'])} chars\")\n",
        "        print(f\"  - Reasoning steps: {len(result['reasoning'])}\")\n",
        "        print(f\"  - Duration: {result['duration']:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Compare Multiple Models with Reasoning\n",
        "\n",
        "Compare Claude, GPT, and Gemini with reasoning enabled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_to_compare = [\"claude-haiku\", \"claude-sonnet\", \"o1-mini\", \"gemini-pro\"]\n",
        "prompt_to_use = \"cultivation\"\n",
        "\n",
        "comparison_results = {}\n",
        "\n",
        "for model in models_to_compare:\n",
        "    if model in REASONING_MODELS:\n",
        "        print(f\"\\n\\nüß™ Testing {model} with reasoning...\\n\")\n",
        "        result = test_with_reasoning(\n",
        "            model_name=model,\n",
        "            user_prompt=WORLD_PROMPTS[prompt_to_use],\n",
        "            effort=\"high\",\n",
        "            max_tokens=5000,\n",
        "            stream=False,\n",
        "            show_reasoning=True,\n",
        "        )\n",
        "        comparison_results[model] = result\n",
        "    else:\n",
        "        print(f\"\\n\\nüß™ Testing {model} (no reasoning)...\\n\")\n",
        "        result = test_model_legacy(\n",
        "            model_name=model,\n",
        "            user_prompt=WORLD_PROMPTS[prompt_to_use],\n",
        "            temperature=0.9,\n",
        "            max_tokens=5000,\n",
        "            stream=False,\n",
        "        )\n",
        "        comparison_results[model] = {\"response\": result, \"reasoning\": [], \"duration\": 0}\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "for model, result in comparison_results.items():\n",
        "    if result:\n",
        "        response = result.get('response', result if isinstance(result, str) else '')\n",
        "        reasoning = result.get('reasoning', []) if isinstance(result, dict) else []\n",
        "        has_spoiler = \"```spoiler\" in str(response)\n",
        "        print(f\"\\n{model}:\")\n",
        "        print(f\"  - Length: {len(str(response))} characters\")\n",
        "        print(f\"  - Has spoiler blocks: {'‚úì' if has_spoiler else '‚úó'}\")\n",
        "        print(f\"  - Reasoning steps: {len(reasoning)}\")\n",
        "        print(f\"  - Preview: {str(response)[:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: Custom Prompt with Reasoning\n",
        "\n",
        "Test your own custom prompt with reasoning enabled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your custom prompt here\n",
        "custom_prompt = \"\"\"\n",
        "Please generate a complete world with the following parameters:\n",
        "- World Type: [Your world type]\n",
        "- Power System: [Your power system]\n",
        "- Starting Class: [Your starting class]\n",
        "- Additional Instructions: [Your custom instructions]\n",
        "\n",
        "Generate the complete story bible as specified in the system prompt, then begin my adventure.\n",
        "\"\"\"\n",
        "\n",
        "# Run test\n",
        "result = test_with_reasoning(\n",
        "    model_name=\"claude-sonnet\",\n",
        "    user_prompt=custom_prompt,\n",
        "    effort=\"high\",\n",
        "    max_tokens=9000,\n",
        "    stream=True,\n",
        "    show_reasoning=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Tools\n",
        "\n",
        "Helper functions to analyze responses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def analyze_response(response: str, reasoning: List[str] = None):\n",
        "    \"\"\"\n",
        "    Analyze a world generation response for key components.\n",
        "    \"\"\"\n",
        "    if not response:\n",
        "        return None\n",
        "    \n",
        "    analysis = {\n",
        "        \"length\": len(response),\n",
        "        \"has_spoiler_blocks\": \"```spoiler\" in response,\n",
        "        \"spoiler_count\": response.count(\"```spoiler\"),\n",
        "        \"has_story_bible\": \"story bible\" in response.lower() or \"section 1\" in response.lower(),\n",
        "        \"has_choices\": any(marker in response.lower() for marker in [\"[a]\", \"[b]\", \"[c]\", \"what do you do\"]),\n",
        "        \"has_character_intro\": any(word in response.lower() for word in [\"you are\", \"your name\", \"your character\"]),\n",
        "        \"reasoning_steps\": len(reasoning) if reasoning else 0,\n",
        "    }\n",
        "    \n",
        "    # Extract spoiler content\n",
        "    spoilers = re.findall(r'```spoiler\\n([\\s\\S]*?)\\n```', response)\n",
        "    analysis[\"spoiler_lengths\"] = [len(s) for s in spoilers]\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "# Example usage:\n",
        "# if result:\n",
        "#     analysis = analyze_response(result['response'], result.get('reasoning'))\n",
        "#     print(json.dumps(analysis, indent=2))\n",
        "\n",
        "print(\"‚úì Analysis tools ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n",
        "\n",
        "Save test results with reasoning data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_test_result(model_name, prompt_name, result, metadata=None):\n",
        "    \"\"\"\n",
        "    Save a test result to a JSON file.\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().isoformat()\n",
        "    filename = f\"test_results/{model_name}_{prompt_name}_{timestamp.replace(':', '-')}.json\"\n",
        "    \n",
        "    os.makedirs(\"test_results\", exist_ok=True)\n",
        "    \n",
        "    # Handle both dict results (with reasoning) and string results (legacy)\n",
        "    if isinstance(result, dict):\n",
        "        response = result.get('response', '')\n",
        "        reasoning = result.get('reasoning', [])\n",
        "    else:\n",
        "        response = result\n",
        "        reasoning = []\n",
        "    \n",
        "    save_data = {\n",
        "        \"timestamp\": timestamp,\n",
        "        \"model\": model_name,\n",
        "        \"prompt_name\": prompt_name,\n",
        "        \"response\": response,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"analysis\": analyze_response(response, reasoning),\n",
        "        \"metadata\": metadata or {},\n",
        "    }\n",
        "    \n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(save_data, f, indent=2)\n",
        "    \n",
        "    print(f\"‚úì Saved to {filename}\")\n",
        "\n",
        "# Example usage:\n",
        "# save_test_result(\"claude-sonnet\", \"medieval_fantasy\", result, {\"effort\": \"high\"})\n",
        "\n",
        "print(\"‚úì Save functions ready!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
